{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rucode_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUgf0T-Pwv8W"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/medium-articles-popularity.zip"
      ],
      "metadata": {
        "id": "sTUP3QkhOB_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "owfsKtR-xWfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "import time\n",
        "import math\n",
        "import matplotlib\n",
        "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "R4YBVb1mxcDk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.random.manual_seed(42)\n",
        "torch.cuda.random.manual_seed(42)\n",
        "torch.cuda.random.manual_seed_all(42)"
      ],
      "metadata": {
        "id": "Fq46a1RwApO_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "metadata": {
        "id": "i-kC-iK7Tfkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/articles_train.csv')\n",
        "data_test=pd.read_csv('/content/articles_test.csv')\n",
        "data = data.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "L29pg02DxzO2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['claps'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXQ-o315U8Ij",
        "outputId": "d771da62-fb23-42e4-c7bd-2c0a5d53a7d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1449.1184771033013"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(3)"
      ],
      "metadata": {
        "id": "SD6EF85zCN4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test.head(3)"
      ],
      "metadata": {
        "id": "ybRCJZRlFAB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "def make_data(data,b=1):\n",
        "  global scaler\n",
        "  if b:\n",
        "    scaler.fit(data['reading_time'][:,None])\n",
        "    data.drop(['id'],axis=1,inplace=True)\n",
        "  data['reading_time']=scaler.transform(data['reading_time'][:,None])\n",
        "  data.drop(['link'],axis=1,inplace=True)\n",
        "  return data"
      ],
      "metadata": {
        "id": "rAWXfe6ugg8W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=make_data(data)\n",
        "data_test=make_data(data_test,0)"
      ],
      "metadata": {
        "id": "s94vTHLRx9gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[0,'text']"
      ],
      "metadata": {
        "id": "gJKnrAhLzgV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "class ArticlesDataset(Dataset):\n",
        "    def __init__(self, data,tokenizer, labels):\n",
        "        self.labels = labels\n",
        "        self.read_time=data.reading_time\n",
        "        self.tokenized = [tokenizer.encode(data.loc[i,'author'])+tokenizer.encode(data.loc[i,'title'])[1:]\\\n",
        "                          +tokenizer.encode(data.loc[i,'text'], max_length=250,truncation=True)[1:] for i in range(data.shape[0])]  \n",
        "    def __getitem__(self, idx):\n",
        "        return {\"tokenized\": self.tokenized[idx],\"read_time\":self.read_time[idx], \"label\": self.labels[idx]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "dataset = ArticlesDataset(data,tokenizer, data['claps'])\n",
        "train_data, valid_data = random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])\n",
        "test_data = ArticlesDataset(data_test,tokenizer, data_test['id'])\n",
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "MzZITBT0cqKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Sampler\n",
        "\n",
        "class ArticlesSampler(Sampler):\n",
        "    def __init__(self, subset,b=1, batch_size=8):\n",
        "        self.batch_size = batch_size\n",
        "        self.subset = subset\n",
        "        if b:\n",
        "          self.indices = subset.indices\n",
        "          self.tokenized = np.array(subset.dataset.tokenized)[self.indices]\n",
        "        else:\n",
        "          self.tokenized= np.array(subset.tokenized)\n",
        "\n",
        "    def __iter__(self):\n",
        "        batch_idx = []\n",
        "        for index in np.argsort(list(map(len, self.tokenized))): # [ len (i) for i in self.tokenized ]\n",
        "            batch_idx.append(index)\n",
        "            if len(batch_idx) == self.batch_size:\n",
        "                yield batch_idx\n",
        "                batch_idx = []\n",
        "\n",
        "        if len(batch_idx) > 0:\n",
        "            yield batch_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "metadata": {
        "id": "_kwWJ391Jq4Y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_padded(values):\n",
        "    max_len = 0\n",
        "    for value in values:\n",
        "        if len(value) > max_len:\n",
        "            max_len = len(value)\n",
        "\n",
        "    padded = np.array([value + [0]*(max_len-len(value)) for value in values])\n",
        "\n",
        "    return padded\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    text = []\n",
        "    labels = []\n",
        "    time = []\n",
        "    for elem in batch:\n",
        "        text.append(elem['tokenized'])\n",
        "        time.append(elem['read_time'])\n",
        "        labels.append(elem['label'])\n",
        "    text = get_padded(text)\n",
        "    attention_mask =np.where(text!=0,1,0) \n",
        "\n",
        "    return {'text': torch.tensor(text), 'labels': torch.FloatTensor(labels),'read_time':torch.tensor(time),\\\n",
        "            'attention_mask' : torch.tensor(attention_mask)}\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_sampler=ArticlesSampler(train_data), collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_data, batch_sampler=ArticlesSampler(valid_data), collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_sampler=ArticlesSampler(test_data,0), collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "HZPi9wXnJ6RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "    def __init__(self, pretrained_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = pretrained_model\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1=nn.Linear(769,1)\n",
        "    def forward(self, text, attention_mask,time):\n",
        "        x=self.dropout(self.relu(model(text,attention_mask=attention_mask)[0][:,0,:]))\n",
        "        time=time[:,None]\n",
        "        x=torch.cat((time,x),axis=1)\n",
        "        x=self.fc1(x)\n",
        "\n",
        "        x=x*1550 \n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "UalMJYi8OM7P"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Bd2DjKWuLwC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bert_clf = BertRegressor(model).to(device,dtype=float)\n",
        "\n",
        "optimizer = optim.Adam(bert_clf.parameters(), lr=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=20, gamma=0.1)\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "cagUA13XX5LD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def warming(model, iterator,num):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "  criterion = nn.MSELoss()\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  k=0\n",
        "  epoh=30\n",
        "  for i in range(epoh):\n",
        "    for j,batch in enumerate(iterator):\n",
        "      batch['text']=batch['text'].to(device)\n",
        "      batch['attention_mask']=batch['attention_mask'].to(device)\n",
        "      batch['read_time']=batch['read_time'].to(device)\n",
        "      output=model(batch['text'],batch['attention_mask'],batch['read_time'])\n",
        "      labels=batch['labels'].to(device,dtype=float)[:,None]\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss = criterion(output, labels) \n",
        "      epoch_loss+=loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ],
      "metadata": {
        "id": "NBfJzkkI5IC8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warming(bert_clf,train_loader,15)"
      ],
      "metadata": {
        "id": "I18ohsy96bDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None):\n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_loss_mse = 0\n",
        "    history = []\n",
        "    for i, batch in enumerate(iterator):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch['text']=batch['text'].to(device)\n",
        "        batch['attention_mask']=batch['attention_mask'].to(device)\n",
        "        batch['read_time']=batch['read_time'].to(device)\n",
        "        output=model(batch['text'],batch['attention_mask'],batch['read_time'])\n",
        "        labels=batch['labels'].to(device,dtype=float)[:,None]\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "      \n",
        "        history.append(loss.cpu().data.numpy())\n",
        "        \n",
        "    return epoch_loss / (i + 1)\n",
        "\n",
        "def evaluate(model, iterator, criterion,mse_loss=nn.MSELoss()):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    history = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            batch['text']=batch['text'].to(device)\n",
        "            batch['attention_mask']=batch['attention_mask'].to(device)\n",
        "            batch['read_time']=batch['read_time'].to(device)\n",
        "\n",
        "            output=model(batch['text'],batch['attention_mask'],batch['read_time'])\n",
        "            labels=batch['labels'].to(device,dtype=float)[:,None]\n",
        "            loss = criterion(output, labels)\n",
        "                      \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_loss_mse=mse_loss(output.detach().to('cpu'),labels.detach().to('cpu'))\n",
        "\n",
        "    return epoch_loss / (i + 1)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "J2h_2CKsRN5-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(suppress=True)"
      ],
      "metadata": {
        "id": "PZowu7zsWDLa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_history = []\n",
        "valid_history = []\n",
        "\n",
        "N_EPOCHS = 50\n",
        "CLIP = 2\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(bert_clf, train_loader, optimizer, criterion, CLIP, train_history, valid_history)\n",
        "    scheduler.step()\n",
        "    valid_loss = evaluate(bert_clf, valid_loader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(bert_clf.state_dict(), 'best-val-model.pt')\n",
        "    \n",
        "    train_history.append(train_loss)\n",
        "    valid_history.append(valid_loss)\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "DH4yRC-sRfgW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = BertRegressor(model).to(device,dtype=float)\n",
        "best_model.load_state_dict(torch.load('/content/drive/MyDrive/best-val-model_0.pt'))"
      ],
      "metadata": {
        "id": "h4zofqrDbRJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}